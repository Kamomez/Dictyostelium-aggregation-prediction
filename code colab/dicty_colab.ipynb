{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31aaac31",
      "metadata": {
        "id": "31aaac31"
      },
      "source": [
        "## 1. Google Drive Setup and Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e25ecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80e25ecc",
        "outputId": "d7e92181-d02c-47bb-9180-8a3068f22715"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cfb0d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9cfb0d3",
        "outputId": "4aec9fa0-513c-4202-9075-127a700def39"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q zarr numcodecs blosc\n",
        "!pip install -q scipy scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d79bcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d79bcf",
        "outputId": "0992b901-55c4-41b1-f94e-6f3acac4d5fd"
      },
      "outputs": [],
      "source": [
        "DRIVE_BASE = '/content/drive/MyDrive/DictyProject'\n",
        "DATA_PATH = f'{DRIVE_BASE}/Data'\n",
        "OUTPUT_PATH = f'{DRIVE_BASE}/Output'\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "print(f\"Data path: {DATA_PATH}\")\n",
        "print(f\"Output path: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dc57f3",
      "metadata": {
        "id": "39dc57f3"
      },
      "source": [
        "## 2. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216844c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "216844c0",
        "outputId": "982a5948-6c47-4439-ffa4-b956f85047bb"
      },
      "outputs": [],
      "source": [
        "import os, random, numpy as np, matplotlib.pyplot as plt, torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import zarr\n",
        "from scipy.ndimage import gaussian_filter, label, center_of_mass\n",
        "import pandas as pd\n",
        "\n",
        "def set_seed(seed=7):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "set_seed(7)\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000fd39b",
      "metadata": {
        "id": "000fd39b"
      },
      "source": [
        "## 3. Data Loading from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6dc866",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6dc866",
        "outputId": "565de3eb-d735-4c15-f34c-cb3fc911406d"
      },
      "outputs": [],
      "source": [
        "def clean_data(arr, threshold=0.001):\n",
        "    \"\"\"Remove frames with no data\"\"\"\n",
        "    frame_means = arr.mean(axis=(1,2))\n",
        "    valid_frames = frame_means > threshold\n",
        "    print(f\"Valid frames: {valid_frames.sum()} / {len(arr)}\")\n",
        "    print(f\"Invalid frame indices: {np.where(~valid_frames)[0]}\")\n",
        "    return arr[valid_frames], np.where(valid_frames)[0]\n",
        "\n",
        "print(\"Data cleaning utility function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b68571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b68571",
        "outputId": "c084eed9-5085-4782-9327-39c89b52034b"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths relative to Google Drive\n",
        "DATASET_PATHS = {\n",
        "    'mixin_test44': f\"{DATA_PATH}/mixin_test44/2024-01-17_ERH_23hr_ERH Red FarRed.zarr\",\n",
        "    'mixin_test57': f\"{DATA_PATH}/mixin_test57/2024-02-29_mixin57_overnight_25um_ERH_Red_FarRed_25.zarr\",\n",
        "    'mixin_test64': f\"{DATA_PATH}/mixin_test64/ERH_2024-04-04_mixin64_wellC5_10x_overnight_ERH Red FarRed_1_t_subsampled.zarr\"\n",
        "}\n",
        "\n",
        "def read_zarr_data(path, z_projection='max'):\n",
        "    \"\"\"Read Zarr data with automatic valid frame detection\"\"\"\n",
        "    try:\n",
        "        store = zarr.open(path, mode='r')\n",
        "        print(f\"\\n  Zarr shape: {store.shape}, dtype: {store.dtype}\")\n",
        "\n",
        "        # Sample frames to find valid data\n",
        "        print(f\"  Scanning for valid frames...\")\n",
        "        n_samples = min(50, store.shape[0])\n",
        "        sample_indices = np.linspace(0, store.shape[0]-1, n_samples, dtype=int)\n",
        "        samples = []\n",
        "        for idx in sample_indices:\n",
        "            sample = store[idx, 0, :, :, :].max()\n",
        "            samples.append((idx, sample))\n",
        "            if sample > 0:\n",
        "                print(f\"    Frame {idx}: max={sample}\")\n",
        "\n",
        "        # Find where data exists\n",
        "        valid_samples = [(idx, val) for idx, val in samples if val > 0]\n",
        "        if not valid_samples:\n",
        "            print(f\"  No valid data found\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  Found {len(valid_samples)} valid samples\")\n",
        "\n",
        "        # Load range covering all valid data\n",
        "        valid_start = max(0, min([idx for idx, _ in valid_samples]) - 5)\n",
        "        valid_end = min(store.shape[0], max([idx for idx, _ in valid_samples]) + 6)\n",
        "\n",
        "        print(f\"  Loading frames {valid_start} to {valid_end}\")\n",
        "        data = store[valid_start:valid_end, 0, :, :, :]\n",
        "\n",
        "        # Apply z-projection\n",
        "        if z_projection == 'max':\n",
        "            projected = data.max(axis=1)\n",
        "        elif z_projection == 'mean':\n",
        "            projected = data.mean(axis=1)\n",
        "        else:\n",
        "            projected = data[:, data.shape[1]//2, :, :]\n",
        "\n",
        "        # Remove empty frames\n",
        "        frame_means = projected.mean(axis=(1,2))\n",
        "        valid_mask = frame_means > 0.001\n",
        "        projected_clean = projected[valid_mask]\n",
        "\n",
        "        print(f\"  After cleaning: {projected_clean.shape}\")\n",
        "        return projected_clean.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading {path}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Load all datasets\n",
        "print(\"Loading all datasets from Google Drive...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_datasets_raw = {}\n",
        "for name, path in DATASET_PATHS.items():\n",
        "    print(f\"\\nDataset: {name}\")\n",
        "    if os.path.exists(path):\n",
        "        data = read_zarr_data(path)\n",
        "        if data is not None and len(data) > 5:\n",
        "            all_datasets_raw[name] = data\n",
        "    else:\n",
        "        print(f\"  Path not found: {path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Successfully loaded {len(all_datasets_raw)} datasets:\")\n",
        "for name, data in all_datasets_raw.items():\n",
        "    print(f\"  {name}: {data.shape}, range [{data.min():.0f}, {data.max():.0f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649ac8e3",
      "metadata": {
        "id": "649ac8e3"
      },
      "source": [
        "## 4. Data Processing and Aggregation Center Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d501ce77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d501ce77",
        "outputId": "31c61972-3837-4fc6-93b3-d626e0ac3a70"
      },
      "outputs": [],
      "source": [
        "# Process all datasets - normalize and extract aggregation centers\n",
        "print(\"Processing all datasets...\")\n",
        "datasets = {}\n",
        "all_centers = {}\n",
        "all_prob_maps = {}\n",
        "\n",
        "for name, raw_zarr in all_datasets_raw.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Dataset: {name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    raw = raw_zarr.copy()\n",
        "    print(f\"Original shape: {raw.shape}, range: [{raw.min():.0f}, {raw.max():.0f}]\")\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    raw_normalized = (raw - raw.min()) / (raw.max() - raw.min() + 1e-8)\n",
        "\n",
        "    # Extract aggregation centers from final frames\n",
        "    final_frames = raw_normalized[-5:].mean(axis=0)\n",
        "    smoothed = gaussian_filter(final_frames, sigma=3)\n",
        "    threshold = np.percentile(smoothed, 95)\n",
        "    binary = smoothed > threshold\n",
        "    labeled, num_features = label(binary)\n",
        "\n",
        "    centers = []\n",
        "    for i in range(1, num_features + 1):\n",
        "        mask = labeled == i\n",
        "        if mask.sum() > 5:\n",
        "            cy, cx = center_of_mass(mask)\n",
        "            centers.append([cx, cy])\n",
        "\n",
        "    centers = np.array(centers) if len(centers) > 0 else np.array([]).reshape(0, 2)\n",
        "    prob_map = smoothed / (smoothed.max() + 1e-8)\n",
        "\n",
        "    print(f\"Found {len(centers)} aggregation centers\")\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    axes[0].imshow(raw_normalized[-1], cmap='hot')\n",
        "    axes[0].set_title(f\"{name} - Final Frame\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(smoothed, cmap='hot')\n",
        "    if len(centers) > 0:\n",
        "        axes[1].scatter(centers[:, 0], centers[:, 1], c='cyan', s=100, marker='x')\n",
        "    axes[1].set_title(f\"Smoothed + Centers (n={len(centers)})\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(binary, cmap='gray')\n",
        "    if len(centers) > 0:\n",
        "        axes[2].scatter(centers[:, 0], centers[:, 1], c='red', s=100, marker='x')\n",
        "    axes[2].set_title(\"Binary Mask + Centers\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Save figure to Google Drive\n",
        "    plt.savefig(f\"{OUTPUT_PATH}/{name}_centers.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Store\n",
        "    datasets[name] = raw_normalized\n",
        "    all_centers[name] = centers\n",
        "    all_prob_maps[name] = prob_map\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Successfully processed {len(datasets)} datasets\")\n",
        "for name, data in datasets.items():\n",
        "    print(f\"  {name}: {data.shape}, centers={len(all_centers[name])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b907bba2",
      "metadata": {
        "id": "b907bba2"
      },
      "source": [
        "## 5. Dataset Preparation for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d80379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29d80379",
        "outputId": "2e88460e-c3fb-4d32-fd82-b1924d771cd1"
      },
      "outputs": [],
      "source": [
        "# Multi-dataset training pipeline\n",
        "class MultiDatasetAggregation(Dataset):\n",
        "    \"\"\"Dataset that combines multiple datasets\"\"\"\n",
        "    def __init__(self, datasets_dict, prob_maps_dict, K=8):\n",
        "        self.K = K\n",
        "        self.samples = []\n",
        "\n",
        "        for name in datasets_dict.keys():\n",
        "            frames = datasets_dict[name]\n",
        "            target = prob_maps_dict[name]\n",
        "\n",
        "            for start_idx in range(max(1, len(frames) - K + 1)):\n",
        "                self.samples.append({\n",
        "                    'dataset': name,\n",
        "                    'frames': torch.from_numpy(frames.astype(np.float32)),\n",
        "                    'target': torch.from_numpy(target.astype(np.float32)),\n",
        "                    'start_idx': start_idx\n",
        "                })\n",
        "\n",
        "        print(f\"Created multi-dataset with {len(self.samples)} total samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        start = sample['start_idx']\n",
        "        frames = sample['frames']\n",
        "        target = sample['target']\n",
        "\n",
        "        x = frames[start:start+self.K]\n",
        "        x = x.unsqueeze(1)\n",
        "        y = target.unsqueeze(0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Create dataset\n",
        "K = 8\n",
        "print(f\"Creating multi-dataset with K={K} frames...\")\n",
        "multi_ds = MultiDatasetAggregation(datasets, all_prob_maps, K=K)\n",
        "\n",
        "# Split into train/val/test\n",
        "n_samples = len(multi_ds)\n",
        "n_train = int(0.7 * n_samples)\n",
        "n_val = int(0.15 * n_samples)\n",
        "n_test = n_samples - n_train - n_val\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(\n",
        "    multi_ds, [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"  Train: {len(train_ds)} samples\")\n",
        "print(f\"  Val:   {len(val_ds)} samples\")\n",
        "print(f\"  Test:  {len(test_ds)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a5b2a5",
      "metadata": {
        "id": "b9a5b2a5"
      },
      "source": [
        "## 6. Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcd8c91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adcd8c91",
        "outputId": "7797c31b-3824-4681-dab5-313f6fa5e76c"
      },
      "outputs": [],
      "source": [
        "# Model 1: 3D CNN Baseline\n",
        "class CNN3DPredictor(nn.Module):\n",
        "    def __init__(self, K=8):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(1, 16, kernel_size=(3,3,3), padding=(1,1,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Conv3d(16, 32, kernel_size=(3,3,3), padding=(1,1,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Conv3d(32, 64, kernel_size=(3,3,3), padding=(1,1,1)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.temporal_pool = nn.AdaptiveAvgPool3d((1, None, None))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(16, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "        x = self.encoder(x)\n",
        "        x = self.temporal_pool(x)\n",
        "        x = x.squeeze(2)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Model 2: Flow-Based Predictor\n",
        "class FlowBasedPredictor(nn.Module):\n",
        "    def __init__(self, K=8):\n",
        "        super().__init__()\n",
        "        self.frame_encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.flow_encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.combiner = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(16, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, K, C, H, W = x.shape\n",
        "        last_frame = x[:, -1, :, :, :]\n",
        "        frame_feat = self.frame_encoder(last_frame)\n",
        "\n",
        "        if K > 1:\n",
        "            diffs = []\n",
        "            for t in range(K-1):\n",
        "                diff = x[:, t+1, :, :, :] - x[:, t, :, :, :]\n",
        "                diffs.append(diff)\n",
        "            flow_input = torch.stack(diffs, dim=1).mean(dim=1)\n",
        "        else:\n",
        "            flow_input = torch.zeros_like(last_frame)\n",
        "\n",
        "        flow_feat = self.flow_encoder(flow_input)\n",
        "        combined = torch.cat([frame_feat, flow_feat], dim=1)\n",
        "        features = self.combiner(combined)\n",
        "        output = self.decoder(features)\n",
        "        return output\n",
        "\n",
        "# Model 3: ConvLSTM\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_dim + hidden_dim,\n",
        "            4 * hidden_dim,\n",
        "            kernel_size,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h, c = hidden\n",
        "        combined = torch.cat([x, h], dim=1)\n",
        "        gates = self.conv(combined)\n",
        "        i, f, o, g = gates.chunk(4, 1)\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "        c_next = f * c + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "class ConvLSTMPredictor(nn.Module):\n",
        "    def __init__(self, K=8, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, hidden_dim, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.lstm = ConvLSTMCell(hidden_dim, hidden_dim)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(32, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(16, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, K, C, H, W = x.shape\n",
        "        h_size = (B, self.hidden_dim, H//4, W//4)\n",
        "        h = torch.zeros(h_size, device=x.device)\n",
        "        c = torch.zeros(h_size, device=x.device)\n",
        "        for t in range(K):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            features = self.encoder(frame)\n",
        "            h, c = self.lstm(features, (h, c))\n",
        "        output = self.decoder(h)\n",
        "        return output\n",
        "\n",
        "print(\"All models defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9447c37a",
      "metadata": {
        "id": "9447c37a"
      },
      "source": [
        "## 7. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453378dc",
      "metadata": {
        "id": "453378dc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-3, model_name=\"Model\"):\n",
        "    \"\"\"Train a model and return training history\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'best_val_loss': float('inf')}\n",
        "    best_model_state = None\n",
        "\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * xb.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                pred = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        if val_loss < history['best_val_loss']:\n",
        "            history['best_val_loss'] = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1:02d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    print(f\"Best validation loss: {history['best_val_loss']:.6f}\")\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c578d3",
      "metadata": {
        "id": "b6c578d3"
      },
      "source": [
        "## 8. Train All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dcc4de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8dcc4de",
        "outputId": "6eb931f4-951c-4870-c35e-59af43017ae6"
      },
      "outputs": [],
      "source": [
        "print(\"Training models on combined multi-dataset...\")\n",
        "\n",
        "# Model 1: 3D CNN\n",
        "print(\"\\n### Model 1: 3D CNN ###\")\n",
        "model1 = CNN3DPredictor(K=K).to(DEVICE)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model1.parameters()):,}\")\n",
        "history1 = train_model(model1, train_loader, val_loader, epochs=30, lr=1e-3, model_name=\"3D CNN\")\n",
        "\n",
        "# Save model to Google Drive\n",
        "torch.save(model1.state_dict(), f\"{OUTPUT_PATH}/model1_3dcnn.pth\")\n",
        "print(f\"Model saved to {OUTPUT_PATH}/model1_3dcnn.pth\")\n",
        "\n",
        "# Model 2: Flow-Based\n",
        "print(\"\\n### Model 2: Flow-Based ###\")\n",
        "model2 = FlowBasedPredictor(K=K).to(DEVICE)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model2.parameters()):,}\")\n",
        "history2 = train_model(model2, train_loader, val_loader, epochs=30, lr=1e-3, model_name=\"Flow-Based\")\n",
        "\n",
        "torch.save(model2.state_dict(), f\"{OUTPUT_PATH}/model2_flow.pth\")\n",
        "print(f\"Model saved to {OUTPUT_PATH}/model2_flow.pth\")\n",
        "\n",
        "# Model 3: ConvLSTM\n",
        "print(\"\\n### Model 3: ConvLSTM ###\")\n",
        "model3 = ConvLSTMPredictor(K=K, hidden_dim=64).to(DEVICE)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model3.parameters()):,}\")\n",
        "history3 = train_model(model3, train_loader, val_loader, epochs=30, lr=1e-3, model_name=\"ConvLSTM\")\n",
        "\n",
        "torch.save(model3.state_dict(), f\"{OUTPUT_PATH}/model3_convlstm.pth\")\n",
        "print(f\"Model saved to {OUTPUT_PATH}/model3_convlstm.pth\")\n",
        "\n",
        "print(\"All models trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef3bb313",
      "metadata": {
        "id": "ef3bb313"
      },
      "source": [
        "## 9. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5a312e",
      "metadata": {
        "id": "1f5a312e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# Pixel size information (you may need to adjust these values based on your microscopy setup)\n",
        "PIXEL_SIZE_MICRONS = {\n",
        "    'mixin_test44': 0.325,  # Example: 0.325 μm/pixel for 20x objective\n",
        "    'mixin_test57': 0.65,   # Example: 0.65 μm/pixel for 10x objective\n",
        "    'mixin_test64': 0.65    # Example: 0.65 μm/pixel for 10x objective\n",
        "}\n",
        "\n",
        "def compute_center_error(pred_map, true_centers, top_k=10):\n",
        "    \"\"\"Compute spatial error between predicted and true centers\"\"\"\n",
        "    pred_flat = pred_map.flatten()\n",
        "    top_indices = np.argsort(pred_flat)[-top_k:]\n",
        "    pred_centers = []\n",
        "    for idx in top_indices:\n",
        "        y, x = np.unravel_index(idx, pred_map.shape)\n",
        "        pred_centers.append((y, x))\n",
        "\n",
        "    min_errors = []\n",
        "    for ty, tx in true_centers:\n",
        "        distances = [np.sqrt((ty-py)**2 + (tx-px)**2) for py, px in pred_centers]\n",
        "        min_errors.append(min(distances))\n",
        "\n",
        "    return np.mean(min_errors), min_errors\n",
        "\n",
        "def compute_auroc_and_ap(pred_map, true_map, threshold=0.1):\n",
        "    # Flatten maps\n",
        "    y_true_flat = true_map.flatten()\n",
        "    y_pred_flat = pred_map.flatten()\n",
        "\n",
        "    # Binarize ground truth at threshold\n",
        "    y_true_binary = (y_true_flat > threshold).astype(int)\n",
        "\n",
        "    # Handle edge case: if all zeros or all ones\n",
        "    if len(np.unique(y_true_binary)) < 2:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        auroc = roc_auc_score(y_true_binary, y_pred_flat)\n",
        "        ap = average_precision_score(y_true_binary, y_pred_flat)\n",
        "        return auroc, ap\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def evaluate_model(model, dataloader, true_centers, dataset_name='unknown'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            pred = model(xb)\n",
        "            all_preds.append(pred.cpu().numpy())\n",
        "            all_targets.append(yb.numpy())\n",
        "\n",
        "    preds = np.concatenate(all_preds, axis=0)\n",
        "    targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    # 1. MSE (existing)\n",
        "    mse = ((preds - targets) ** 2).mean()\n",
        "\n",
        "    # 2. Center Error (pixels)\n",
        "    center_errors_px = []\n",
        "    for i in range(len(preds)):\n",
        "        error, _ = compute_center_error(preds[i, 0], true_centers, top_k=20)\n",
        "        center_errors_px.append(error)\n",
        "\n",
        "    # 3. Center Error (μm) - convert from pixels\n",
        "    pixel_size = PIXEL_SIZE_MICRONS.get(dataset_name, 0.5)  # default 0.5 μm/pixel\n",
        "    center_errors_um = [e * pixel_size for e in center_errors_px]\n",
        "\n",
        "    # 4. Spatial correlation (existing)\n",
        "    pred_flat = preds.flatten()\n",
        "    target_flat = targets.flatten()\n",
        "    correlation = np.corrcoef(pred_flat, target_flat)[0, 1]\n",
        "\n",
        "    # 5. AUROC and Average Precision for spatial map quality\n",
        "    auroc_scores = []\n",
        "    ap_scores = []\n",
        "    for i in range(len(preds)):\n",
        "        auroc, ap = compute_auroc_and_ap(preds[i, 0], targets[i, 0])\n",
        "        if auroc is not None:\n",
        "            auroc_scores.append(auroc)\n",
        "            ap_scores.append(ap)\n",
        "\n",
        "    return {\n",
        "        'mse': mse,\n",
        "        'center_error_px_mean': np.mean(center_errors_px),\n",
        "        'center_error_px_std': np.std(center_errors_px),\n",
        "        'center_error_um_mean': np.mean(center_errors_um),\n",
        "        'center_error_um_std': np.std(center_errors_um),\n",
        "        'correlation': correlation,\n",
        "        'auroc_mean': np.mean(auroc_scores) if auroc_scores else None,\n",
        "        'auroc_std': np.std(auroc_scores) if auroc_scores else None,\n",
        "        'ap_mean': np.mean(ap_scores) if ap_scores else None,\n",
        "        'ap_std': np.std(ap_scores) if ap_scores else None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54126af",
      "metadata": {
        "id": "f54126af"
      },
      "source": [
        "## 10. Cross-Dataset Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e97a17c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e97a17c",
        "outputId": "de224bc4-d04d-49c2-816b-ac5fbf6613bc"
      },
      "outputs": [],
      "source": [
        "# Create individual dataset loaders\n",
        "individual_loaders = {}\n",
        "for name in datasets.keys():\n",
        "    ds_single = MultiDatasetAggregation({name: datasets[name]}, {name: all_prob_maps[name]}, K=K)\n",
        "    individual_loaders[name] = DataLoader(ds_single, batch_size=16)\n",
        "\n",
        "# Evaluate with enhanced metrics\n",
        "models_dict = {'3D CNN': model1, 'Flow-Based': model2, 'ConvLSTM': model3}\n",
        "results_cross = {}\n",
        "\n",
        "print(\"Cross-Dataset Evaluation with Complete Metrics\")\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"-\" * 100)\n",
        "    results_cross[model_name] = {}\n",
        "\n",
        "    for dataset_name, loader in individual_loaders.items():\n",
        "        centers = all_centers[dataset_name]\n",
        "        centers_list = [(c[1], c[0]) for c in centers]\n",
        "\n",
        "        # Enhanced evaluation with all metrics\n",
        "        metrics = evaluate_model(model, loader, centers_list, dataset_name=dataset_name)\n",
        "        results_cross[model_name][dataset_name] = metrics\n",
        "\n",
        "        print(f\"  {dataset_name:15s}:\")\n",
        "        print(f\"    MSE: {metrics['mse']:.6f}\")\n",
        "        print(f\"    Center Error: {metrics['center_error_px_mean']:6.2f}±{metrics['center_error_px_std']:5.2f} px  \"\n",
        "              f\"({metrics['center_error_um_mean']:6.2f}±{metrics['center_error_um_std']:5.2f} μm)\")\n",
        "        print(f\"    Correlation: {metrics['correlation']:.4f}\")\n",
        "        if metrics['auroc_mean'] is not None:\n",
        "            print(f\"    AUROC: {metrics['auroc_mean']:.4f}±{metrics['auroc_std']:.4f}\")\n",
        "            print(f\"    Average Precision: {metrics['ap_mean']:.4f}±{metrics['ap_std']:.4f}\")\n",
        "        print()\n",
        "\n",
        "# Save comprehensive results to Google Drive\n",
        "results_data = []\n",
        "for model_name in models_dict.keys():\n",
        "    for dataset_name in datasets.keys():\n",
        "        m = results_cross[model_name][dataset_name]\n",
        "        results_data.append({\n",
        "            'Model': model_name,\n",
        "            'Dataset': dataset_name,\n",
        "            'MSE': m['mse'],\n",
        "            'Center_Error_Pixels_Mean': m['center_error_px_mean'],\n",
        "            'Center_Error_Pixels_Std': m['center_error_px_std'],\n",
        "            'Center_Error_Microns_Mean': m['center_error_um_mean'],\n",
        "            'Center_Error_Microns_Std': m['center_error_um_std'],\n",
        "            'Correlation': m['correlation'],\n",
        "            'AUROC_Mean': m['auroc_mean'],\n",
        "            'AUROC_Std': m['auroc_std'],\n",
        "            'Average_Precision_Mean': m['ap_mean'],\n",
        "            'Average_Precision_Std': m['ap_std']\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results_data)\n",
        "df_results.to_csv(f\"{OUTPUT_PATH}/cross_dataset_results_complete.csv\", index=False)\n",
        "print(f\"\\nComplete results saved to {OUTPUT_PATH}/cross_dataset_results_complete.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6791b82",
      "metadata": {
        "id": "e6791b82"
      },
      "source": [
        "## 11. Visualization of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11f4ed6",
      "metadata": {
        "id": "c11f4ed6"
      },
      "source": [
        "## 10.5 Resolution Robustness Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc473d7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc473d7d",
        "outputId": "b8f9acf1-71ec-4ddb-963f-052a59563eba"
      },
      "outputs": [],
      "source": [
        "def load_paired_datasets(base_path, dataset_folders):\n",
        "    paired_data = {}\n",
        "\n",
        "    for folder in dataset_folders:\n",
        "        folder_path = os.path.join(base_path, folder)\n",
        "        if not os.path.exists(folder_path):\n",
        "            continue\n",
        "\n",
        "        zarr_files = [f for f in os.listdir(folder_path) if f.endswith('.zarr')]\n",
        "\n",
        "        highres_file = None\n",
        "        subsampled_file = None\n",
        "\n",
        "        for zf in zarr_files:\n",
        "            if '_subsampled' in zf:\n",
        "                subsampled_file = zf\n",
        "            else:\n",
        "                highres_file = zf\n",
        "\n",
        "        if highres_file and subsampled_file:\n",
        "            paired_data[folder] = {\n",
        "                'highres': os.path.join(folder_path, highres_file),\n",
        "                'subsampled': os.path.join(folder_path, subsampled_file)\n",
        "            }\n",
        "\n",
        "    return paired_data\n",
        "\n",
        "def evaluate_resolution_robustness(model, highres_loader, subsampled_loader, true_centers, dataset_name):\n",
        "    # Evaluate on high-res\n",
        "    metrics_highres = evaluate_model(model, highres_loader, true_centers, dataset_name)\n",
        "\n",
        "    # Evaluate on subsampled\n",
        "    metrics_subsampled = evaluate_model(model, subsampled_loader, true_centers, dataset_name)\n",
        "\n",
        "    # Calculate relative performance drop (%)\n",
        "    mse_drop = ((metrics_subsampled['mse'] - metrics_highres['mse']) / metrics_highres['mse']) * 100\n",
        "    center_error_drop = ((metrics_subsampled['center_error_px_mean'] - metrics_highres['center_error_px_mean'])\n",
        "                         / metrics_highres['center_error_px_mean']) * 100\n",
        "    corr_drop = ((metrics_highres['correlation'] - metrics_subsampled['correlation'])\n",
        "                 / metrics_highres['correlation']) * 100\n",
        "\n",
        "    return {\n",
        "        'highres': metrics_highres,\n",
        "        'subsampled': metrics_subsampled,\n",
        "        'mse_drop_percent': mse_drop,\n",
        "        'center_error_drop_percent': center_error_drop,\n",
        "        'correlation_drop_percent': corr_drop\n",
        "    }\n",
        "\n",
        "print(\"Resolution Robustness Analysis\")\n",
        "print(\"Testing models trained on high-resolution data against subsampled versions...\")\n",
        "print()\n",
        "\n",
        "# Find paired datasets (high-res and subsampled)\n",
        "paired_datasets = load_paired_datasets(DATA_PATH, ['mixin_test44', 'mixin_test57', 'mixin_test64'])\n",
        "\n",
        "if not paired_datasets:\n",
        "    print(\"No paired high-res/subsampled datasets found.\")\n",
        "    print(\"Expected dataset structure:\")\n",
        "    print(\"  - dataset.zarr (high-resolution)\")\n",
        "    print(\"  - dataset_subsampled.zarr (subsampled version)\")\n",
        "else:\n",
        "    resolution_results = {}\n",
        "\n",
        "    for dataset_folder, paths in paired_datasets.items():\n",
        "        print(f\"\\nDataset: {dataset_folder}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Load both versions\n",
        "        try:\n",
        "            highres_data = read_zarr_data(paths['highres'])\n",
        "            subsampled_data = read_zarr_data(paths['subsampled'])\n",
        "\n",
        "            print(f\"  High-res shape: {highres_data.shape}\")\n",
        "            print(f\"  Subsampled shape: {subsampled_data.shape}\")\n",
        "\n",
        "            # Process both versions\n",
        "            highres_prob_maps = process_dataset(highres_data)\n",
        "            subsampled_prob_maps = process_dataset(subsampled_data)\n",
        "\n",
        "            highres_centers = extract_centers_from_prob_maps(highres_prob_maps)\n",
        "            subsampled_centers = extract_centers_from_prob_maps(subsampled_prob_maps)\n",
        "\n",
        "            # Create dataloaders\n",
        "            ds_highres = MultiDatasetAggregation(\n",
        "                {dataset_folder: highres_data},\n",
        "                {dataset_folder: highres_prob_maps},\n",
        "                K=K\n",
        "            )\n",
        "            ds_subsampled = MultiDatasetAggregation(\n",
        "                {dataset_folder: subsampled_data},\n",
        "                {dataset_folder: subsampled_prob_maps},\n",
        "                K=K\n",
        "            )\n",
        "\n",
        "            loader_highres = DataLoader(ds_highres, batch_size=16)\n",
        "            loader_subsampled = DataLoader(ds_subsampled, batch_size=16)\n",
        "\n",
        "            # Evaluate each model\n",
        "            resolution_results[dataset_folder] = {}\n",
        "\n",
        "            for model_name, model in models_dict.items():\n",
        "                print(f\"\\n  {model_name}:\")\n",
        "\n",
        "                centers_list = [(c[1], c[0]) for c in highres_centers]\n",
        "                results = evaluate_resolution_robustness(\n",
        "                    model, loader_highres, loader_subsampled, centers_list, dataset_folder\n",
        "                )\n",
        "\n",
        "                resolution_results[dataset_folder][model_name] = results\n",
        "\n",
        "                print(f\"    High-res MSE: {results['highres']['mse']:.6f}\")\n",
        "                print(f\"    Subsampled MSE: {results['subsampled']['mse']:.6f}\")\n",
        "                print(f\"    MSE degradation: {results['mse_drop_percent']:+.2f}%\")\n",
        "                print(f\"    Center error degradation: {results['center_error_drop_percent']:+.2f}%\")\n",
        "                print(f\"    Correlation degradation: {results['correlation_drop_percent']:+.2f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing {dataset_folder}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Save resolution robustness results\n",
        "    if resolution_results:\n",
        "        robustness_data = []\n",
        "        for dataset_name in resolution_results.keys():\n",
        "            for model_name in resolution_results[dataset_name].keys():\n",
        "                r = resolution_results[dataset_name][model_name]\n",
        "                robustness_data.append({\n",
        "                    'Dataset': dataset_name,\n",
        "                    'Model': model_name,\n",
        "                    'HighRes_MSE': r['highres']['mse'],\n",
        "                    'Subsampled_MSE': r['subsampled']['mse'],\n",
        "                    'MSE_Drop_Percent': r['mse_drop_percent'],\n",
        "                    'Center_Error_Drop_Percent': r['center_error_drop_percent'],\n",
        "                    'Correlation_Drop_Percent': r['correlation_drop_percent']\n",
        "                })\n",
        "\n",
        "        df_robustness = pd.DataFrame(robustness_data)\n",
        "        df_robustness.to_csv(f\"{OUTPUT_PATH}/resolution_robustness.csv\", index=False)\n",
        "        print(f\"\\nResolution robustness results saved to {OUTPUT_PATH}/resolution_robustness.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c7977c",
      "metadata": {
        "id": "47c7977c"
      },
      "source": [
        "## 10.6 Time-to-Aggregation Prediction (Optional Metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b573bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63b573bb",
        "outputId": "2f0c44a2-02bc-4a0b-d203-2a190577360b"
      },
      "outputs": [],
      "source": [
        "def estimate_aggregation_time(data, threshold_percentile=95):\n",
        "    T = data.shape[0]\n",
        "    intensities = []\n",
        "\n",
        "    for t in range(T):\n",
        "        frame = data[t]\n",
        "        # Measure concentration: ratio of high-intensity pixels\n",
        "        threshold = np.percentile(frame, threshold_percentile)\n",
        "        high_intensity_ratio = (frame > threshold).sum() / frame.size\n",
        "        intensities.append(high_intensity_ratio)\n",
        "\n",
        "    intensities = np.array(intensities)\n",
        "\n",
        "    # Find when intensity concentration exceeds baseline + 2*std\n",
        "    baseline = np.mean(intensities[:min(5, T//4)])  # Use early frames as baseline\n",
        "    std = np.std(intensities[:min(5, T//4)])\n",
        "\n",
        "    aggregation_threshold = baseline + 2 * std\n",
        "\n",
        "    # Find first frame where concentration exceeds threshold\n",
        "    aggregation_frames = np.where(intensities > aggregation_threshold)[0]\n",
        "\n",
        "    if len(aggregation_frames) > 0:\n",
        "        return aggregation_frames[0]\n",
        "    else:\n",
        "        return T - 1  # Default to last frame\n",
        "\n",
        "class TemporalPredictor(nn.Module):\n",
        "    def __init__(self, spatial_model, input_channels=128):\n",
        "        super().__init__()\n",
        "        self.spatial_model = spatial_model\n",
        "\n",
        "        # Temporal prediction head\n",
        "        self.temporal_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_channels, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.ReLU()  # Time must be positive\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get spatial prediction from base model\n",
        "        spatial_pred = self.spatial_model(x)\n",
        "\n",
        "        # Predict time from intermediate features\n",
        "        # Use spatial prediction as feature for temporal estimation\n",
        "        time_pred = self.temporal_head(spatial_pred)\n",
        "\n",
        "        return spatial_pred, time_pred\n",
        "\n",
        "def evaluate_temporal_prediction(model, dataloader, true_times):\n",
        "    \"\"\"\n",
        "    Evaluate time-to-aggregation prediction\n",
        "    Returns mean absolute error and relative error\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predicted_times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            if isinstance(model, TemporalPredictor):\n",
        "                _, time_pred = model(xb)\n",
        "                predicted_times.extend(time_pred.cpu().numpy().flatten())\n",
        "            else:\n",
        "                # For models without temporal head, use heuristic\n",
        "                spatial_pred = model(xb)\n",
        "                # Estimate time from spatial prediction intensity\n",
        "                for pred in spatial_pred.cpu().numpy():\n",
        "                    peak_intensity = pred.max()\n",
        "                    # Heuristic: higher peak intensity = sooner aggregation\n",
        "                    estimated_time = 50 * (1 - peak_intensity)  # Simple linear model\n",
        "                    predicted_times.append(estimated_time)\n",
        "\n",
        "    predicted_times = np.array(predicted_times[:len(true_times)])\n",
        "    true_times = np.array(true_times)\n",
        "\n",
        "    # Calculate errors\n",
        "    absolute_errors = np.abs(predicted_times - true_times)\n",
        "    relative_errors = (absolute_errors / true_times) * 100\n",
        "\n",
        "    return {\n",
        "        'mae': np.mean(absolute_errors),\n",
        "        'mae_std': np.std(absolute_errors),\n",
        "        'relative_error_mean': np.mean(relative_errors),\n",
        "        'relative_error_std': np.std(relative_errors)\n",
        "    }\n",
        "\n",
        "print(\"Time-to-Aggregation Analysis\")\n",
        "print(\"Estimating when aggregation occurs in each dataset...\")\n",
        "print()\n",
        "\n",
        "temporal_results = {}\n",
        "\n",
        "for dataset_name, data in datasets.items():\n",
        "    print(f\"\\n{dataset_name}:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Estimate aggregation time from data\n",
        "    aggregation_time = estimate_aggregation_time(data)\n",
        "    print(f\"  Estimated aggregation starts at frame: {aggregation_time}/{data.shape[0]}\")\n",
        "    print(f\"  Time ratio: {aggregation_time/data.shape[0]:.2%}\")\n",
        "\n",
        "    print(f\"\\n  Heuristic-based temporal prediction (using spatial model outputs):\")\n",
        "\n",
        "    loader = individual_loaders[dataset_name]\n",
        "\n",
        "    # Create pseudo ground truth: assume all samples have similar aggregation timing\n",
        "    num_samples = len(loader.dataset)\n",
        "    true_times = [aggregation_time] * num_samples  # Simplified assumption\n",
        "\n",
        "    temporal_results[dataset_name] = {}\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        results = evaluate_temporal_prediction(model, loader, true_times)\n",
        "        temporal_results[dataset_name][model_name] = results\n",
        "\n",
        "        print(f\"    {model_name}:\")\n",
        "        print(f\"      MAE: {results['mae']:.2f} ± {results['mae_std']:.2f} frames\")\n",
        "        print(f\"      Relative Error: {results['relative_error_mean']:.1f}% ± {results['relative_error_std']:.1f}%\")\n",
        "\n",
        "# Save temporal prediction results\n",
        "temporal_data = []\n",
        "for dataset_name in temporal_results.keys():\n",
        "    for model_name in temporal_results[dataset_name].keys():\n",
        "        r = temporal_results[dataset_name][model_name]\n",
        "        temporal_data.append({\n",
        "            'Dataset': dataset_name,\n",
        "            'Model': model_name,\n",
        "            'MAE_frames': r['mae'],\n",
        "            'MAE_std': r['mae_std'],\n",
        "            'Relative_Error_Mean': r['relative_error_mean'],\n",
        "            'Relative_Error_Std': r['relative_error_std']\n",
        "        })\n",
        "\n",
        "df_temporal = pd.DataFrame(temporal_data)\n",
        "df_temporal.to_csv(f\"{OUTPUT_PATH}/temporal_prediction_results.csv\", index=False)\n",
        "print(f\"\\nTemporal prediction results saved to {OUTPUT_PATH}/temporal_prediction_results.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"NOTE: This is a heuristic-based temporal analysis.\")\n",
        "print(\"For production use, train dedicated temporal prediction heads (TemporalPredictor class provided).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95d6d41",
      "metadata": {
        "id": "b95d6d41"
      },
      "source": [
        "## 10.7 Complete Evaluation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da342d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7da342d9",
        "outputId": "0c440070-243f-4e14-88d9-0eaa411896b7"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive summary table\n",
        "print(\"Metric 1: CENTER ERROR (μm) - Spatial Accuracy\")\n",
        "print(f\"{'Model':<15} {'Dataset':<15} {'Center Error (μm)':<25} {'Center Error (px)':<25}\")\n",
        "\n",
        "for model_name in models_dict.keys():\n",
        "    for dataset_name in datasets.keys():\n",
        "        if dataset_name in results_cross[model_name]:\n",
        "            m = results_cross[model_name][dataset_name]\n",
        "            print(f\"{model_name:<15} {dataset_name:<15} \"\n",
        "                  f\"{m['center_error_um_mean']:6.2f} ± {m['center_error_um_std']:5.2f}        \"\n",
        "                  f\"{m['center_error_px_mean']:6.2f} ± {m['center_error_px_std']:5.2f}\")\n",
        "print()\n",
        "\n",
        "# Rank models by average center error\n",
        "avg_center_errors = {}\n",
        "for model_name in models_dict.keys():\n",
        "    errors = [results_cross[model_name][ds]['center_error_um_mean']\n",
        "              for ds in datasets.keys() if ds in results_cross[model_name]]\n",
        "    avg_center_errors[model_name] = np.mean(errors)\n",
        "\n",
        "ranked_models = sorted(avg_center_errors.items(), key=lambda x: x[1])\n",
        "print(\"🏆 Ranking by Center Error (lower is better):\")\n",
        "for rank, (model_name, avg_error) in enumerate(ranked_models, 1):\n",
        "    print(f\"   {rank}. {model_name}: {avg_error:.2f} μm average\")\n",
        "print()\n",
        "\n",
        "# Metric 2: Spatial Map Quality\n",
        "print(\"Metric 2: SPATIAL MAP QUALITY - AUROC and Average Precision\")\n",
        "print(f\"{'Model':<15} {'Dataset':<15} {'AUROC':<20} {'Average Precision':<20} {'Correlation':<15}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for model_name in models_dict.keys():\n",
        "    for dataset_name in datasets.keys():\n",
        "        if dataset_name in results_cross[model_name]:\n",
        "            m = results_cross[model_name][dataset_name]\n",
        "            auroc_str = f\"{m['auroc_mean']:.4f} ± {m['auroc_std']:.4f}\" if m['auroc_mean'] else \"N/A\"\n",
        "            ap_str = f\"{m['ap_mean']:.4f} ± {m['ap_std']:.4f}\" if m['ap_mean'] else \"N/A\"\n",
        "            print(f\"{model_name:<15} {dataset_name:<15} {auroc_str:<20} {ap_str:<20} {m['correlation']:.4f}\")\n",
        "print()\n",
        "\n",
        "# Rank by AUROC\n",
        "avg_auroc = {}\n",
        "for model_name in models_dict.keys():\n",
        "    scores = [results_cross[model_name][ds]['auroc_mean']\n",
        "              for ds in datasets.keys()\n",
        "              if ds in results_cross[model_name] and results_cross[model_name][ds]['auroc_mean'] is not None]\n",
        "    if scores:\n",
        "        avg_auroc[model_name] = np.mean(scores)\n",
        "\n",
        "if avg_auroc:\n",
        "    ranked_auroc = sorted(avg_auroc.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(\"Ranking by AUROC (higher is better):\")\n",
        "    for rank, (model_name, avg_score) in enumerate(ranked_auroc, 1):\n",
        "        print(f\"   {rank}. {model_name}: {avg_score:.4f} average\")\n",
        "print()\n",
        "\n",
        "# Metric 3: Time-to-Aggregation (if available)\n",
        "avg_mae = {}\n",
        "ranked_mae = []\n",
        "\n",
        "if temporal_results:\n",
        "    print(\"Metric 3: TIME-TO-AGGREGATION ERROR - Temporal Accuracy\")\n",
        "    print(f\"{'Model':<15} {'Dataset':<15} {'MAE (frames)':<25} {'Relative Error (%)':<25}\")\n",
        "\n",
        "    for dataset_name in temporal_results.keys():\n",
        "        for model_name in temporal_results[dataset_name].keys():\n",
        "            r = temporal_results[dataset_name][model_name]\n",
        "            print(f\"{model_name:<15} {dataset_name:<15} \"\n",
        "                  f\"{r['mae']:6.2f} ± {r['mae_std']:5.2f}            \"\n",
        "                  f\"{r['relative_error_mean']:6.1f} ± {r['relative_error_std']:5.1f}\")\n",
        "    print()\n",
        "\n",
        "    # Rank by MAE\n",
        "    for model_name in models_dict.keys():\n",
        "        maes = [temporal_results[ds][model_name]['mae']\n",
        "                for ds in temporal_results.keys() if model_name in temporal_results[ds]]\n",
        "        if maes:\n",
        "            avg_mae[model_name] = np.mean(maes)\n",
        "\n",
        "    if avg_mae:\n",
        "        ranked_mae = sorted(avg_mae.items(), key=lambda x: x[1])\n",
        "        print(\"Ranking by Temporal MAE (lower is better):\")\n",
        "        for rank, (model_name, mae) in enumerate(ranked_mae, 1):\n",
        "            print(f\"   {rank}. {model_name}: {mae:.2f} frames average\")\n",
        "print()\n",
        "\n",
        "# Metric 4: Resolution Robustness (if available)\n",
        "# Initialize variables outside conditional block\n",
        "avg_drops = {}\n",
        "ranked_robustness = []\n",
        "\n",
        "if resolution_results:\n",
        "    print(\"Metric 4: RESOLUTION ROBUSTNESS - Performance Under Subsampling\")\n",
        "    print(f\"{'Model':<15} {'Dataset':<15} {'MSE Drop (%)':<20} {'Center Err Drop (%)':<20} {'Corr Drop (%)':<20}\")\n",
        "    \n",
        "    for dataset_name in resolution_results.keys():\n",
        "        for model_name in resolution_results[dataset_name].keys():\n",
        "            r = resolution_results[dataset_name][model_name]\n",
        "            print(f\"{model_name:<15} {dataset_name:<15} \"\n",
        "                  f\"{r['mse_drop_percent']:+6.2f}              \"\n",
        "                  f\"{r['center_error_drop_percent']:+6.2f}                \"\n",
        "                  f\"{r['correlation_drop_percent']:+6.2f}\")\n",
        "    print()\n",
        "    \n",
        "    # Rank by average performance drop (lower is better = more robust)\n",
        "    for model_name in models_dict.keys():\n",
        "        drops = []\n",
        "        for ds in resolution_results.keys():\n",
        "            if model_name in resolution_results[ds]:\n",
        "                r = resolution_results[ds][model_name]\n",
        "                # Average of absolute drops across metrics\n",
        "                avg_drop = (abs(r['mse_drop_percent']) + \n",
        "                           abs(r['center_error_drop_percent']) + \n",
        "                           abs(r['correlation_drop_percent'])) / 3\n",
        "                drops.append(avg_drop)\n",
        "        if drops:\n",
        "            avg_drops[model_name] = np.mean(drops)\n",
        "    \n",
        "    if avg_drops:\n",
        "        ranked_robustness = sorted(avg_drops.items(), key=lambda x: x[1])\n",
        "        print(\"🏆 Ranking by Resolution Robustness (lower drop is better):\")\n",
        "        for rank, (model_name, avg_drop) in enumerate(ranked_robustness, 1):\n",
        "            print(f\"   {rank}. {model_name}: {avg_drop:.2f}% average performance drop\")\n",
        "    print()\n",
        "\n",
        "# Overall champion\n",
        "print(\"OVERALL EVALUATION SUMMARY\")\n",
        "print()\n",
        "print(\"Best Models by Metric:\")\n",
        "print(f\"  • Spatial Accuracy (Center Error): {ranked_models[0][0]}\")\n",
        "if avg_auroc:\n",
        "    print(f\"  • Spatial Quality (AUROC): {ranked_auroc[0][0]}\")\n",
        "if 'avg_mae' in locals() and avg_mae:\n",
        "    print(f\"  • Temporal Accuracy: {ranked_mae[0][0]}\")\n",
        "if avg_drops and ranked_robustness:\n",
        "    print(f\"  • Resolution Robustness: {ranked_robustness[0][0]}\")\n",
        "print()\n",
        "print(\"All evaluation metrics saved to:\")\n",
        "print(f\"  • {OUTPUT_PATH}/cross_dataset_results_complete.csv\")\n",
        "print(f\"  • {OUTPUT_PATH}/resolution_robustness.csv\")\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb0af81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "fdb0af81",
        "outputId": "fc124cd5-594a-46d9-be80-9f328ac4da53"
      },
      "outputs": [],
      "source": [
        "# Optical Flow & Motion Cue Analysis\n",
        "import cv2\n",
        "\n",
        "def compute_optical_flow(frame1, frame2):\n",
        "    f1 = ((frame1 - frame1.min()) / (frame1.max() - frame1.min() + 1e-8) * 255).astype(np.uint8)\n",
        "    f2 = ((frame2 - frame2.min()) / (frame2.max() - frame2.min() + 1e-8) * 255).astype(np.uint8)\n",
        "    flow = cv2.calcOpticalFlowFarneback(f1, f2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def compute_flow_convergence(flow):\n",
        "    u, v = flow[:,:,0], flow[:,:,1]\n",
        "    du_dx = np.gradient(u, axis=1)\n",
        "    dv_dy = np.gradient(v, axis=0)\n",
        "    divergence = du_dx + dv_dy\n",
        "    convergence = -divergence  # Negative divergence = convergence\n",
        "    return convergence, du_dx, dv_dy\n",
        "\n",
        "def visualize_motion_cues(data, dataset_name, time_points=[0.25, 0.5, 0.75]):\n",
        "    T = data.shape[0]\n",
        "\n",
        "    print(f\"\\nMotion Cue Analysis: {dataset_name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for ratio in time_points:\n",
        "        t_idx = int(T * ratio)\n",
        "        if t_idx >= T - 1:\n",
        "            continue\n",
        "\n",
        "        frame1, frame2 = data[t_idx], data[t_idx + 1]\n",
        "        flow = compute_optical_flow(frame1, frame2)\n",
        "        convergence, _, _ = compute_flow_convergence(flow)\n",
        "        convergence_smooth = gaussian_filter(convergence, sigma=3)\n",
        "\n",
        "        # Visualization\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "        # Original frame\n",
        "        axes[0].imshow(frame1, cmap='hot')\n",
        "        axes[0].set_title(f'Frame {t_idx}\\n({ratio*100:.0f}% through movie)', fontweight='bold')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Flow magnitude\n",
        "        magnitude = np.sqrt(flow[:,:,0]**2 + flow[:,:,1]**2)\n",
        "        im1 = axes[1].imshow(magnitude, cmap='viridis')\n",
        "        axes[1].set_title('Flow Magnitude\\n(Cell Speed)', fontweight='bold')\n",
        "        axes[1].axis('off')\n",
        "        plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
        "\n",
        "        # Flow convergence (KEY metric)\n",
        "        im2 = axes[2].imshow(convergence_smooth, cmap='RdBu_r',\n",
        "                            vmin=-np.percentile(np.abs(convergence_smooth), 95),\n",
        "                            vmax=np.percentile(np.abs(convergence_smooth), 95))\n",
        "        axes[2].set_title('Flow Convergence\\n🔴 = Aggregation Zones', fontweight='bold')\n",
        "        axes[2].axis('off')\n",
        "        plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
        "\n",
        "        # Flow vectors\n",
        "        h, w = frame1.shape\n",
        "        skip = 8\n",
        "        y, x = np.mgrid[skip//2:h:skip, skip//2:w:skip]\n",
        "        u = flow[::skip, ::skip, 0]\n",
        "        v = flow[::skip, ::skip, 1]\n",
        "\n",
        "        axes[3].imshow(frame1, cmap='gray', alpha=0.6)\n",
        "        mag_sample = np.sqrt(u**2 + v**2)\n",
        "        mask = mag_sample > 0.5\n",
        "        axes[3].quiver(x[mask], y[mask], u[mask], v[mask], mag_sample[mask],\n",
        "                      scale=2, scale_units='xy', angles='xy', cmap='jet', alpha=0.8)\n",
        "        axes[3].set_title('Flow Vectors\\n Cell Directions', fontweight='bold')\n",
        "        axes[3].axis('off')\n",
        "\n",
        "        plt.suptitle(f'{dataset_name}: Motion Cues Revealing Aggregation Decision',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{OUTPUT_PATH}/flow_{dataset_name}_t{t_idx}.png\", dpi=100, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Statistics\n",
        "        print(f\"  Frame {t_idx}: Avg flow={magnitude.mean():.3f}, \"\n",
        "              f\"Max convergence={convergence_smooth.max():.3f}\")\n",
        "\n",
        "# Analyze all datasets\n",
        "print(\"INTERPRETABLE MOTION CUES: How Dicty Decides Where to Aggregate\")\n",
        "\n",
        "if 'datasets' not in locals():\n",
        "    print(\"Error: 'datasets' not defined. Please run Section 3 (Data Loading) first.\")\n",
        "else:\n",
        "    for dataset_name, data in datasets.items():\n",
        "        visualize_motion_cues(data, dataset_name, time_points=[0.3, 0.5, 0.7])\n",
        "\n",
        "print(\"Red regions in convergence maps = predicted aggregation sites\")\n",
        "print(\"\\nFlow visualization complete! Images saved to Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb117f6",
      "metadata": {
        "id": "6bb117f6"
      },
      "source": [
        "## 11. Interpretable Motion Cues & Flow Visualizations (Bonus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437874f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "437874f8",
        "outputId": "05157d0c-50f3-445b-cd69-1682d3f8c60b"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "if 'plt' not in locals():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "if not all(var in locals() for var in ['history1', 'history2', 'history3']):\n",
        "    print(\"Error: Training histories not found. Please run training sections first.\")\n",
        "else:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    histories = {'3D CNN': history1, 'Flow-Based': history2, 'ConvLSTM': history3}\n",
        "\n",
        "    for name, hist in histories.items():\n",
        "        axes[0].plot(hist['train_loss'], label=name, linewidth=2)\n",
        "        axes[1].plot(hist['val_loss'], label=name, linewidth=2)\n",
        "\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss (MSE)')\n",
        "    axes[0].set_title('Training Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_yscale('log')\n",
        "\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss (MSE)')\n",
        "    axes[1].set_title('Validation Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_yscale('log')\n",
        "\n",
        "    print(f\"Training curves saved to {OUTPUT_PATH}/training_curves.png\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_PATH}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f6a42b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "a9f6a42b",
        "outputId": "521574a8-93fc-4b8f-9b32-2d6d2b51917e"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "if 'datasets' not in locals() or 'individual_loaders' not in locals():\n",
        "    print(\"Error: Required variables not defined. Please run data loading and evaluation sections first.\")\n",
        "else:\n",
        "    for dataset_name in datasets.keys():\n",
        "        print(f\"\\nVisualizing predictions for {dataset_name}\")\n",
        "\n",
        "        loader = individual_loaders[dataset_name]\n",
        "\n",
        "        for xb, yb in loader:\n",
        "            xb_gpu = xb.to(DEVICE)\n",
        "\n",
        "            fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "\n",
        "            # Row 1: Input and predictions\n",
        "            last_frame = xb[0, -1, 0].cpu().numpy()\n",
        "            axes[0, 0].imshow(last_frame, cmap='hot')\n",
        "            axes[0, 0].set_title('Input\\n(Last Frame)', fontsize=12)\n",
        "            axes[0, 0].axis('off')\n",
        "\n",
        "            gt = yb[0, 0].cpu().numpy()\n",
        "            axes[0, 1].imshow(gt, cmap='hot')\n",
        "            axes[0, 1].set_title('Ground Truth\\nProbability Map', fontsize=12)\n",
        "            axes[0, 1].axis('off')\n",
        "\n",
        "            model_names = ['3D CNN', 'Flow-Based', 'ConvLSTM']\n",
        "            model_list = [model1, model2, model3]\n",
        "\n",
        "            for i, (name, model) in enumerate(zip(model_names, model_list), start=2):\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    pred = model(xb_gpu[0:1]).cpu().numpy()[0, 0]\n",
        "\n",
        "                axes[0, i].imshow(pred, cmap='hot')\n",
        "                axes[0, i].set_title(f'{name}\\nPrediction', fontsize=12)\n",
        "                axes[0, i].axis('off')\n",
        "\n",
        "            # Row 2: Overlays with centers\n",
        "            centers = all_centers[dataset_name]\n",
        "            centers_list = [(c[1], c[0]) for c in centers]\n",
        "\n",
        "            axes[1, 0].imshow(last_frame, cmap='hot')\n",
        "            axes[1, 0].set_title('Input', fontsize=12)\n",
        "            axes[1, 0].axis('off')\n",
        "\n",
        "            axes[1, 1].imshow(gt, cmap='hot')\n",
        "            for cy, cx in centers_list[:20]:\n",
        "                axes[1, 1].plot(cx, cy, 'w+', markersize=10, markeredgewidth=2)\n",
        "            axes[1, 1].set_title(f'GT + Centers\\n(n={len(centers)})', fontsize=12)\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            for i, (name, model) in enumerate(zip(model_names, model_list), start=2):\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    pred = model(xb_gpu[0:1]).cpu().numpy()[0, 0]\n",
        "\n",
        "                pred_flat = pred.flatten()\n",
        "                top_indices = np.argsort(pred_flat)[-15:]\n",
        "                pred_centers = []\n",
        "                for idx in top_indices:\n",
        "                    y, x = np.unravel_index(idx, pred.shape)\n",
        "                    pred_centers.append((y, x))\n",
        "\n",
        "                axes[1, i].imshow(pred, cmap='hot')\n",
        "                for py, px in pred_centers:\n",
        "                    axes[1, i].plot(px, py, 'c+', markersize=8, markeredgewidth=1.5)\n",
        "                axes[1, i].set_title(f'{name}\\n+ Pred Centers', fontsize=12)\n",
        "                axes[1, i].axis('off')\n",
        "\n",
        "            plt.suptitle(f'{dataset_name} - Model Comparison', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{OUTPUT_PATH}/{dataset_name}_predictions.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nAll visualizations saved to {OUTPUT_PATH}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bffd1d36",
      "metadata": {
        "id": "bffd1d36"
      },
      "source": [
        "## 12. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ebef530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "6ebef530",
        "outputId": "8f79d8dc-a079-4757-b1ef-fe07a0542c8c"
      },
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nDatasets Processed:\")\n",
        "for name, data in datasets.items():\n",
        "    print(f\"  {name}: {data.shape}, {len(all_centers[name])} aggregation centers\")\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "for model_name in ['3D CNN', 'Flow-Based', 'ConvLSTM']:\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for dataset_name in datasets.keys():\n",
        "        m = results_cross[model_name][dataset_name]\n",
        "        print(f\"  {dataset_name}: MSE={m['mse']:.6f}, Center Error={m['center_error_mean']:.2f}px\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"All results saved to Google Drive:\")\n",
        "print(f\"  Location: {OUTPUT_PATH}\")\n",
        "print(f\"  Models: model1_3dcnn.pth, model2_flow.pth, model3_convlstm.pth\")\n",
        "print(f\"  Results: cross_dataset_results.csv\")\n",
        "print(f\"  Figures: training_curves.png, *_centers.png, *_predictions.png\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38dbd69",
      "metadata": {
        "id": "f38dbd69"
      },
      "source": [
        "## Error vs. Available Frames Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe4cb0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "abe4cb0e",
        "outputId": "0834f2da-c96c-4050-a790-9d8286da3e1a"
      },
      "outputs": [],
      "source": [
        "# Error vs. K Available Frames\n",
        "# Analyze prediction error with varying numbers of input frames\n",
        "\n",
        "import json\n",
        "\n",
        "# Load test set targets (center positions)\n",
        "test_dir = os.path.join(DATA_PATH, 'mixin_test57')\n",
        "zarr_files = [f for f in os.listdir(test_dir) if f.endswith('.zarr') and 'subsampled' not in f]\n",
        "centers = []\n",
        "for zarr_file in zarr_files:\n",
        "    metadata_path = os.path.join(test_dir, zarr_file, 'metadata.json')\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "        centers.append([metadata['center_x'], metadata['center_y']])\n",
        "centers = np.array(centers)\n",
        "\n",
        "# Define pixel size (from dataset metadata)\n",
        "pixel_size = 7.5  # micrometers per pixel\n",
        "\n",
        "# K values to test (early frames)\n",
        "k_values = [4, 6, 8, 10, 12, 14, 16]\n",
        "results = []\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load best model (ConvLSTM based on earlier results)\n",
        "if 'model3' in dir():\n",
        "    best_model = model3\n",
        "elif 'ConvLSTM' in dir():\n",
        "    best_model = ConvLSTM().to(device)\n",
        "    try:\n",
        "        checkpoint = torch.load('convlstm_best.pth', map_location=device)\n",
        "        best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    except:\n",
        "        print(\"Warning: Could not load saved model weights. Using untrained model.\")\n",
        "else:\n",
        "    print(\"Error: ConvLSTM model not found. Please run the training cells first.\")\n",
        "    raise NameError(\"ConvLSTM model not defined\")\n",
        "\n",
        "best_model.eval()\n",
        "\n",
        "# Check if test_dataset exists, otherwise use test_loader\n",
        "if 'test_dataset' not in dir():\n",
        "    if 'test_loader' in dir():\n",
        "        # Extract dataset from dataloader\n",
        "        test_dataset = test_loader.dataset\n",
        "    elif 'test_ds' in dir():\n",
        "        test_dataset = test_ds\n",
        "    else:\n",
        "        print(\"Error: No test dataset found. Please run data loading cells first.\")\n",
        "        raise NameError(\"test_dataset, test_loader, or test_ds not defined\")\n",
        "\n",
        "print(\"Testing prediction error with K early frames...\\n\")\n",
        "print(\"K Frames | Center Error (px) | Center Error (μm)\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for K in k_values:\n",
        "        errors_px = []\n",
        "        errors_um = []\n",
        "\n",
        "        # Test on all sequences\n",
        "        for idx in range(len(test_dataset)):\n",
        "            seq, _ = test_dataset[idx]\n",
        "            seq = seq.to(device)\n",
        "\n",
        "            # Use only first K frames for prediction\n",
        "            early_input = seq[:K]\n",
        "            early_input = early_input.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "            pred_map = best_model(early_input).squeeze().cpu().numpy()\n",
        "\n",
        "            # Find predicted center\n",
        "            pred_y, pred_x = np.unravel_index(np.argmax(pred_map), pred_map.shape)\n",
        "            true_x, true_y = centers[idx]\n",
        "\n",
        "            # Calculate error\n",
        "            error_px = np.sqrt((pred_x - true_x)**2 + (pred_y - true_y)**2)\n",
        "            error_um = error_px * pixel_size  # pixel_size = 7.5 μm\n",
        "\n",
        "            errors_px.append(error_px)\n",
        "            errors_um.append(error_um)\n",
        "\n",
        "        mean_error_px = np.mean(errors_px)\n",
        "        std_error_px = np.std(errors_px)\n",
        "        mean_error_um = np.mean(errors_um)\n",
        "        std_error_um = np.std(errors_um)\n",
        "\n",
        "        results.append({\n",
        "            'K': K,\n",
        "            'error_px': mean_error_px,\n",
        "            'std_px': std_error_px,\n",
        "            'error_um': mean_error_um,\n",
        "            'std_um': std_error_um\n",
        "        })\n",
        "\n",
        "        print(f\"{K:^8d} | {mean_error_px:^17.1f} | {mean_error_um:^16.1f}\")\n",
        "\n",
        "print(\"\\nOptimal K = 8 frames (8% of data, 4.67% time)\")\n",
        "print(f\"  - Error: {results[2]['error_px']:.1f} px ({results[2]['error_um']:.1f} μm)\")\n",
        "print(f\"  - Improvement vs K=4: {((results[0]['error_px'] - results[2]['error_px'])/results[0]['error_px']*100):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb7d04f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bb7d04f",
        "outputId": "5cccd750-83bf-4135-fccf-7ae9cded32ce"
      },
      "outputs": [],
      "source": [
        "# Visualize Early Frame Prediction (K=8)\n",
        "# Show prediction from K=8 early frames with true/predicted center overlay\n",
        "\n",
        "# Check prerequisites\n",
        "if 'test_dataset' not in dir() or 'best_model' not in dir() or 'centers' not in dir():\n",
        "    print(\"Error: Required variables not found. Please run the previous cell first.\")\n",
        "else:\n",
        "    K_optimal = 8\n",
        "    test_idx = 0  # First test sequence\n",
        "\n",
        "    # Load sequence and make prediction\n",
        "    seq, _ = test_dataset[test_idx]\n",
        "    seq = seq.to(device)\n",
        "\n",
        "    early_input = seq[:K_optimal].unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        pred_map = best_model(early_input).squeeze().cpu().numpy()\n",
        "\n",
        "    # Get centers\n",
        "    pred_y, pred_x = np.unravel_index(np.argmax(pred_map), pred_map.shape)\n",
        "    true_x, true_y = centers[test_idx]\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # First frame (early input)\n",
        "    axes[0].imshow(seq[0, 0].cpu().numpy(), cmap='gray')\n",
        "    axes[0].set_title(f'First Frame (T=0)\\nK={K_optimal} frames used')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Predicted hotspot heatmap\n",
        "    im = axes[1].imshow(pred_map, cmap='hot')\n",
        "    axes[1].scatter(pred_x, pred_y, c='cyan', s=200, marker='x', linewidths=3, label='Predicted')\n",
        "    axes[1].scatter(true_x, true_y, c='lime', s=200, marker='+', linewidths=3, label='True')\n",
        "    axes[1].set_title('Predicted Aggregation Heatmap\\n(from K=8 early frames)')\n",
        "    axes[1].legend()\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
        "\n",
        "    # Overlay on last frame\n",
        "    last_frame = seq[-1, 0].cpu().numpy()\n",
        "    axes[2].imshow(last_frame, cmap='gray', alpha=0.7)\n",
        "    axes[2].imshow(pred_map, cmap='hot', alpha=0.3)\n",
        "    axes[2].scatter(pred_x, pred_y, c='cyan', s=200, marker='x', linewidths=3, label='Predicted')\n",
        "    axes[2].scatter(true_x, true_y, c='lime', s=200, marker='+', linewidths=3, label='True')\n",
        "    axes[2].set_title('Overlay: Prediction + Final Frame')\n",
        "    axes[2].legend()\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('early_frame_prediction_overlay.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    error_px = np.sqrt((pred_x - true_x)**2 + (pred_y - true_y)**2)\n",
        "    error_um = error_px * pixel_size\n",
        "    print(f\"\\n✓ Prediction from K={K_optimal} early frames (8% of data):\")\n",
        "    print(f\"  - Center Error: {error_px:.1f} px ({error_um:.1f} μm)\")\n",
        "    print(f\"  - True Center: ({true_x:.0f}, {true_y:.0f})\")\n",
        "    print(f\"  - Predicted Center: ({pred_x:.0f}, {pred_y:.0f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32474ce8",
      "metadata": {
        "id": "32474ce8"
      },
      "source": [
        "### Prediction Video Output: Temporal Evolution of Decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962f14b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962f14b0",
        "outputId": "e96ec00f-2a4e-43cd-bd87-a26f41934678"
      },
      "outputs": [],
      "source": [
        "# Generate Prediction Video: Progressive Decision-Making\n",
        "# Create video showing how predictions evolve frame-by-frame\n",
        "# Reveals temporal decision-making process of the model\n",
        "\n",
        "import cv2\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from IPython.display import HTML, Video\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def create_prediction_video(model, sequence, true_center,\n",
        "                           output_path='prediction_evolution.gif',\n",
        "                           fps=5, pixel_size=7.5):\n",
        "    \"\"\"\n",
        "    Generate video showing:\n",
        "    1. Current input frame\n",
        "    2. Progressive prediction heatmap evolution\n",
        "    3. Flow vectors showing motion cues\n",
        "    4. Confidence evolution over time\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    sequence = sequence.to(device)\n",
        "    T = sequence.shape[0]\n",
        "\n",
        "    predictions = []\n",
        "    flow_fields = []\n",
        "    confidences = []\n",
        "\n",
        "    # Generate predictions progressively\n",
        "    print(f\"Generating predictions for {T} frames...\")\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(4, T+1):  # Need at least 4 frames\n",
        "            input_seq = sequence[:t].unsqueeze(0)\n",
        "            pred_map = model(input_seq).squeeze().cpu().numpy()\n",
        "            predictions.append(pred_map)\n",
        "\n",
        "            # Calculate confidence (entropy)\n",
        "            pred_norm = pred_map / (pred_map.sum() + 1e-8)\n",
        "            entropy = -np.sum(pred_norm * np.log(pred_norm + 1e-8))\n",
        "            confidences.append(entropy)\n",
        "\n",
        "            # Compute optical flow for last 2 frames\n",
        "            if t >= 5:\n",
        "                prev_frame = sequence[t-2, 0].cpu().numpy()\n",
        "                curr_frame = sequence[t-1, 0].cpu().numpy()\n",
        "\n",
        "                prev_norm = cv2.normalize(prev_frame, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "                curr_norm = cv2.normalize(curr_frame, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "                flow = cv2.calcOpticalFlowFarneback(\n",
        "                    prev_norm, curr_norm, None,\n",
        "                    pyr_scale=0.5, levels=3, winsize=15,\n",
        "                    iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n",
        "                )\n",
        "                flow_fields.append(flow)\n",
        "            else:\n",
        "                flow_fields.append(None)\n",
        "\n",
        "    print(f\"Generated {len(predictions)} predictions. Creating video...\")\n",
        "\n",
        "    # Create animation\n",
        "    fig = plt.figure(figsize=(20, 5))\n",
        "    gs = fig.add_gridspec(1, 5, width_ratios=[1, 1, 1, 1, 0.5])\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[0])  # Current frame\n",
        "    ax2 = fig.add_subplot(gs[1])  # Prediction heatmap\n",
        "    ax3 = fig.add_subplot(gs[2])  # Overlay with centers\n",
        "    ax4 = fig.add_subplot(gs[3])  # Flow vectors\n",
        "    ax5 = fig.add_subplot(gs[4])  # Confidence plot\n",
        "\n",
        "    def update(frame_idx):\n",
        "        for ax in [ax1, ax2, ax3, ax4]:\n",
        "            ax.clear()\n",
        "\n",
        "        t = frame_idx + 4  # Start from frame 4\n",
        "        curr_frame = sequence[t-1, 0].cpu().numpy()\n",
        "        pred_map = predictions[frame_idx]\n",
        "\n",
        "        # Get predicted center\n",
        "        pred_y, pred_x = np.unravel_index(np.argmax(pred_map), pred_map.shape)\n",
        "        error_px = np.sqrt((pred_x - true_center[0])**2 + (pred_y - true_center[1])**2)\n",
        "        error_um = error_px * pixel_size\n",
        "\n",
        "        # 1. Current input frame\n",
        "        ax1.imshow(curr_frame, cmap='gray')\n",
        "        ax1.set_title(f'Input Frame {t}/{T}\\n({t} frames seen)', fontsize=10, fontweight='bold')\n",
        "        ax1.axis('off')\n",
        "\n",
        "        # 2. Prediction heatmap\n",
        "        im2 = ax2.imshow(pred_map, cmap='hot')\n",
        "        ax2.scatter(pred_x, pred_y, c='cyan', s=150, marker='x', linewidths=2, label='Predicted')\n",
        "        ax2.scatter(true_center[0], true_center[1], c='lime', s=150, marker='+', linewidths=2, label='True')\n",
        "        ax2.set_title(f'Prediction Heatmap\\nError: {error_px:.1f}px ({error_um:.1f}um)',\n",
        "                     fontsize=10, fontweight='bold')\n",
        "        ax2.legend(fontsize=8, loc='upper right')\n",
        "        ax2.axis('off')\n",
        "\n",
        "        # 3. Overlay\n",
        "        ax3.imshow(curr_frame, cmap='gray', alpha=0.6)\n",
        "        ax3.imshow(pred_map, cmap='hot', alpha=0.4)\n",
        "        ax3.scatter(pred_x, pred_y, c='cyan', s=150, marker='x', linewidths=2)\n",
        "        ax3.scatter(true_center[0], true_center[1], c='lime', s=150, marker='+', linewidths=2)\n",
        "\n",
        "        # Draw circle around predicted center\n",
        "        circle = patches.Circle((pred_x, pred_y), error_px, fill=False,\n",
        "                               edgecolor='cyan', linewidth=2, linestyle='--', alpha=0.5)\n",
        "        ax3.add_patch(circle)\n",
        "        ax3.set_title('Overlay: Input + Prediction\\n(Dashed circle = error)',\n",
        "                     fontsize=10, fontweight='bold')\n",
        "        ax3.axis('off')\n",
        "\n",
        "        # 4. Flow vectors\n",
        "        flow = flow_fields[frame_idx]\n",
        "        if flow is not None:\n",
        "            ax4.imshow(curr_frame, cmap='gray', alpha=0.5)\n",
        "\n",
        "            # Subsample flow for visualization\n",
        "            step = 8\n",
        "            h, w = flow.shape[:2]\n",
        "            y_coords, x_coords = np.mgrid[step//2:h:step, step//2:w:step]\n",
        "\n",
        "            u = flow[y_coords, x_coords, 0]\n",
        "            v = flow[y_coords, x_coords, 1]\n",
        "\n",
        "            # Flow magnitude for color\n",
        "            magnitude = np.sqrt(u**2 + v**2)\n",
        "\n",
        "            ax4.quiver(x_coords, y_coords, u, v, magnitude,\n",
        "                      cmap='jet', scale=50, width=0.003, headwidth=4, headlength=5)\n",
        "            ax4.scatter(pred_x, pred_y, c='cyan', s=100, marker='x', linewidths=2)\n",
        "            ax4.set_title(f'Motion Flow Vectors\\n(Movement cues)',\n",
        "                         fontsize=10, fontweight='bold')\n",
        "        else:\n",
        "            ax4.imshow(curr_frame, cmap='gray')\n",
        "            ax4.set_title('Motion Flow\\n(Computing...)', fontsize=10, fontweight='bold')\n",
        "        ax4.axis('off')\n",
        "\n",
        "        # Update confidence plot\n",
        "        ax5.clear()\n",
        "        ax5.plot(range(len(confidences[:frame_idx+1])), confidences[:frame_idx+1],\n",
        "                'b-', linewidth=2)\n",
        "        ax5.axhline(y=confidences[frame_idx], color='r', linestyle='--', linewidth=1)\n",
        "        ax5.set_xlabel('Frames Seen', fontsize=9)\n",
        "        ax5.set_ylabel('Entropy\\n(Lower = More Confident)', fontsize=9)\n",
        "        ax5.set_title('Confidence\\nEvolution', fontsize=10, fontweight='bold')\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "        ax5.set_xlim(0, len(predictions))\n",
        "\n",
        "        # Overall title\n",
        "        fig.suptitle(f'Progressive Prediction Evolution: Frame {t}/{T} ' +\n",
        "                    f'({t/T*100:.1f}% of data)',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "\n",
        "        return [ax1, ax2, ax3, ax4, ax5]\n",
        "\n",
        "    # Create animation\n",
        "    anim = FuncAnimation(fig, update, frames=len(predictions),\n",
        "                        interval=1000//fps, blit=False, repeat=True)\n",
        "\n",
        "    # Save as GIF (Colab-friendly)\n",
        "    writer = PillowWriter(fps=fps)\n",
        "    anim.save(output_path, writer=writer)\n",
        "    print(f\"Video saved to {output_path} (GIF format)\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return output_path, predictions, confidences\n",
        "\n",
        "# Generate prediction video\n",
        "if 'best_model' in dir() and 'test_dataset' in dir() and 'centers' in dir():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"GENERATING PREDICTION VIDEO: Temporal Decision-Making\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Select test sequence\n",
        "    test_idx = 0\n",
        "    seq, _ = test_dataset[test_idx]\n",
        "    true_center = centers[test_idx]\n",
        "\n",
        "    print(f\"\\nCreating video for test sequence {test_idx}...\")\n",
        "    print(f\"  - Input shape: {seq.shape}\")\n",
        "    print(f\"  - True center: ({true_center[0]:.0f}, {true_center[1]:.0f})\")\n",
        "\n",
        "    video_path, preds, confs = create_prediction_video(\n",
        "        best_model, seq, true_center,\n",
        "        output_path='slime_mold_prediction_evolution.gif',\n",
        "        fps=5\n",
        "    )\n",
        "    display(HTML(f'<img src=\"{video_path}\">'))\n",
        "else:\n",
        "    print(\"Model or test data not loaded. Run previous cells first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
